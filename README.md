This project implements a structured data preprocessing pipeline focused on missing value detection, analysis, and treatment using Python-based data science libraries. Two datasets from different domains were utilized to validate the robustness of the cleaning workflow. The first dataset, a medical dataset loaded from the scikit-learn library, contains fully numerical features and is originally free of missing values; therefore, controlled artificial missingness was introduced to emulate real-world data acquisition issues such as incomplete records or sensor failures. The second dataset represents a structured tabular dataset from a general or housing-related domain, enabling the application of similar preprocessing techniques across varied feature distributions.

Missing values were quantified using Pandasâ€™ .isnull().sum() method and visualized through bar plots to assess missingness patterns across features. Feature-wise imputation strategies were applied based on data type and statistical properties: median imputation was used for numerical variables to reduce sensitivity to outliers, while mode imputation was employed for categorical variables to preserve the most frequent category. Features exceeding a predefined missing-value threshold were removed to prevent noise amplification and model bias. Post-processing validation ensured complete removal of missing values and maintained dataset integrity. The cleaned datasets were persisted as CSV files for downstream machine learning workflows, emphasizing reproducibility, data quality assurance, and preprocessing best practices.
